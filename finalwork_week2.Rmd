---
title: "WEEK2 Report"
author: "ADK"
date: "8 de mayo de 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r functionsandvariables, echo=FALSE, warning=FALSE}
library("tm")
library("ngram")
library("ggplot2")
library("knitr")

scriptwd = "F:/Data Science/Curso/trabajoFinal/scripts/finalassignment/"
datawd ="F:/Data Science/Curso/trabajoFinal/Datos/final/en_US/"
twitterfile = "en_US.twitter.txt"
blogfile ="en_US.blogs.txt"
newsfile = "en_US.news.txt"

readgivenfile <- function(lines = 10000,file){
      linestoread <- lines
      con <- file(paste(datawd,file, sep = ""), "r")
      linestoprocess <- NULL 
      if (linestoread ==-1) linestoprocess <- readLines(con)
          else linestoprocess <- readLines(con,linestoread)
      close(con)
      ptd <- PlainTextDocument(linestoprocess)
      return(ptd)
}

bigramtokenizer <-  function(x){
  unlist(lapply(ngrams(words(x), 2), 
                paste, 
                collapse = " "), 
         use.names = FALSE)
}

trigramtokenizer <-  function(x){
  unlist(lapply(ngrams(words(x), 3), 
                paste, 
                collapse = " "), 
         use.names = FALSE)
}

index50and90percent <- function(wordfrecuency){
      totalsinglewords <- length(wordfrecuency)
      totaldocwords <- sum(wordfrecuency)
      threshold.50 = 0.5
      threshold.90 = 0.9
      countedwords = 0
      iteratorindex = 0
      
      index <- NULL
      while (countedwords <= threshold.50 * totaldocwords){
        iteratorindex <- iteratorindex + 1
        countedwords  <- countedwords + wordfrecuency[iteratorindex]
      }
      percent50 <-  iteratorindex
      
      while (countedwords <= threshold.90 * totaldocwords){
        iteratorindex <- iteratorindex + 1
        countedwords  <- countedwords + wordfrecuency[iteratorindex]
      }
      percent90 <-  iteratorindex
      return(c(percent50,percent90))
}

saveinfo <- function(object, file){
  outfile = paste("F:/Data Science/Curso/trabajoFinal/Datos/",file, sep="")
  write.table(x = object, outfile)
}

```

## Summary

This report explain some conclusion of the exploratory analysis of the characteristics of the 3 files, considered as input for the final assigment.
Also, it shall explain some basics of the proposed prediction algorithm.

## Exploratory analysis

Considering the given data, it can be seen the following facts:

```{r process_twitter, echo = FALSE,cache=TRUE}

ptd <- readgivenfile(10000,file=twitterfile)
wordfrecuency <-  termFreq(doc=ptd, control = list(removeNumbers = TRUE,
                                                    tolower = TRUE,
                                                    removePunctuation = TRUE))
wordfrecuency <- wordfrecuency[order(wordfrecuency, decreasing = TRUE)]

saveinfo(wordfrecuency,"twitter_wordfrecuency.dat")

twitter_mostusedwords <- as.data.frame(wordfrecuency[1:3])
twitter_mostusedwords <-     paste(twitter_mostusedwords[,1]," (",twitter_mostusedwords[,2],")", sep="")
twitter_index <- index50and90percent(wordfrecuency)
twitter_usedwords <- length(wordfrecuency)
twitter_totalwords <- sum(wordfrecuency)

twitter_graph <- qplot(data=as.data.frame(wordfrecuency)[1:10,], x=Var1,y=Freq, xlab="words", main = "Words vs Freq - first 10 words") +
  geom_density()


twowordfrecuency <-  termFreq(doc=ptd,control = list(removeNumbers = TRUE,
                                             tolower = TRUE,
                                             tokenize = bigramtokenizer,
                                             removePunctuation = TRUE)
                      )

twowordfrecuency <- as.data.frame(twowordfrecuency[order(twowordfrecuency,decreasing = TRUE)],stringsAsFactors = FALSE)

saveinfo(twowordfrecuency,"twitter_twowordfrecuency.dat")

twowordfrecuency <- twowordfrecuency[1:5,]

threewordfrecuency <-  termFreq(doc=ptd,control = list(removeNumbers = TRUE,
                                             tolower = TRUE,
                                             tokenize = trigramtokenizer,
                                             removePunctuation = TRUE)
)
threewordfrecuency <-as.data.frame(threewordfrecuency[order(threewordfrecuency,decreasing = TRUE)], stringsAsFactors = FALSE)
saveinfo(threewordfrecuency,"twitter_threewordfrecuency.dat")

threewordfrecuency <- threewordfrecuency[1:5,]

wordfrecuency <- NULL
ptd <- NULL
```


```{r process_blog, echo = FALSE, cache=TRUE}

ptd <- readgivenfile(10000,file=blogfile)
wordfrecuency <-  termFreq(doc=ptd, control = list(removeNumbers = TRUE,
                                                    tolower = TRUE,
                                                    removePunctuation = TRUE))
wordfrecuency <- wordfrecuency[order(wordfrecuency, decreasing = TRUE)]
saveinfo(wordfrecuency,"blog_wordfrecuency.dat")

blog_mostusedwords <- as.data.frame(wordfrecuency[1:3])
blog_mostusedwords <- paste(blog_mostusedwords[,1]," (",blog_mostusedwords[,2],")", sep="")
blog_index <- index50and90percent(wordfrecuency)
blog_usedwords <- length(wordfrecuency)
blog_totalwords <- sum(wordfrecuency)

wordfrecuency <- NULL
ptd <- NULL
```

```{r process_news, echo = FALSE, cache=TRUE, warning=FALSE}

ptd <- readgivenfile(lines=-1,file=newsfile)
wordfrecuency <-  termFreq(doc=ptd, control = list(removeNumbers = TRUE,
                                                    tolower = TRUE,
                                                    removePunctuation = TRUE))
wordfrecuency <- wordfrecuency[order(wordfrecuency, decreasing = TRUE)]
saveinfo(wordfrecuency,"news_wordfrecuency.dat")

news_mostusedwords <- as.data.frame(wordfrecuency[1:3])
news_mostusedwords <-     paste(news_mostusedwords[,1]," (",news_mostusedwords[,2],")", sep="")
news_index <- index50and90percent(wordfrecuency)
news_usedwords <- length(wordfrecuency)
news_totalwords <- sum(wordfrecuency)

wordfrecuency <- NULL
ptd <- NULL

```



### The most used words in each file are:


``` {r table_mostusedwords, echo = FALSE}
mostusedwords <- data.frame(twitter_mostusedwords
                            , news_mostusedwords
                            , blog_mostusedwords)

kable( mostusedwords , caption = "Most used words per file"
       ##, align = c('l', 'c', 'r', 'r', 'c', 'l')
       , col.names = c("Twitter file", "News file","Blog file")
       , row.names = TRUE
       , digits = 1
       , format.args = list( decimal.mark = ",")
      )
```

If we consider the 10 most used words in the *Twitter file*, we can see that its rapidly reduced the amount of different words across the document:

```{r twittergraph, fig.width=5, echo=FALSE}
twitter_graph
```

the following shows the number of words that represents the 90% of the total number of words, and the percentage of it in comparison with all the words considered in the file:


* *Twitter file*: `r twitter_index[2]` different words out of `r twitter_usedwords` words, which means the `r twitter_index[2]*100/twitter_usedwords`% of different words represents the 90% of the total of words.

* *Blog file*: `r blog_index[2]` different words out of `r blog_usedwords` words, which means the `r blog_index[2]*100/blog_usedwords`% of different words represents the 90% of the total of words.

* *News file*: `r news_index[2]` different words out of `r news_usedwords` words, which means the `r news_index[2]*100/news_usedwords`% of different words represents the 90% of the total of words.

### n-grams information

Regarding the 2-grams and 3-grams in the twitter file, we can stat that the 5 most used are:

``` {r table_mostusedngrmas, echo = FALSE}

twowordfrecuency <- paste(twowordfrecuency[,1]," (",twowordfrecuency[,2],")", sep="")

threewordfrecuency <- paste(threewordfrecuency[,1]," (",threewordfrecuency[,2],")", sep="")

mostusedngram <- data.frame(twowordfrecuency
                            ,threewordfrecuency)

kable( mostusedngram , caption = "5 most used n-grams in Twitter file"
       ##, align = c('l', 'c', 'r', 'r', 'c', 'l')
       , col.names = c("2-gram", "3-gram")
       , row.names = TRUE
       , digits = 1
       , format.args = list( decimal.mark = ",")
      )
```


## Words about prediction algorithm

The model will be stored in a decision tree, where each level shall represent 
the 1-gram, 2-gram and 3-grams that could be seen in the training files, and the order shall be defined as it can be seen in the those files.  
As an example, if we have the following 2-grams:

* "for the"
* "for you"
* "as you"
* "as soon"
* "all"

So, the first level of the tree shall consider the words *for*, *all* and *as*, and the order shall be defined by the frecuency of the word in the text.  
The second level of the tree, considering the leaf *for*, shall consider the words *the* and *you*, and its order shall be defined by taking into account that the most frecuent of the 2-gram *for the* and *for you* goes first in the prediction.

Continuing with the example, let's consider the following data:

``` {r table_example, echo = FALSE}


text <- c("for"
          ,"as"
          ,"all"
          ,"for you"
          ,"for the"
          ,"as you"
          ,"as soon")

freq <- c(1000, 950, 100,200,500,100,45)


data <- data.frame(text, freq)

kable( data , caption = "Algorithm example"
       ##, align = c('l', 'c', 'r', 'r', 'c', 'l')
       , col.names = c("N-gram", "Frecuency")
       , row.names = TRUE
       , digits = 1
       , format.args = list( decimal.mark = ",")
      )
```

At first, before any letter is added by the algorithm user, only the word *for* shall be suggested. Once the user adds *f*,*fo* or *for*, the suggestion shall be *"for the"*. On the other hand, if the user adds *a* or *as*, the suggestion shall be *"as you"*.  

## Additional

It was considered only up to 10.000 lines in each file, due to computational limitations.

